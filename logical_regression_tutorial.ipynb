{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Your task in this assignment is to predict the ethnicity of someone's DNA, based on single nucleotide polymorphism data we've shared with you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data values\n",
    "\n",
    "Each input vector represents the DNA at specific locations in the genome for one individual. There are 20 binary input features. 0 indicates that the user's DNA at the given location matches the human reference genome. 1 indicates that the user's DNA does not match the human reference genome. The output class value represents the super population (ethnicity) of each individual. The super populations contained in this dataset are East Asian or Mixed American, encoded in binary. The training data set contains 283 data vectors, and the testing data set contains 184 data vectors.\n",
    "\n",
    "We will do this in three different ways. Firstly, (Part A) we will do it visually by exmaining the graphs of the distributions and trying to guess which input features are likely to be relevant in predicting ethnicityThen we write If..Else statements to classify manually. Next (Part B), we replace our visual inspection with a correelation map and use a threshold that can be changed to see the trade-off between FPs and FNs. Lastly (Part C), we will use a pre-existing library for Logistic Regression to demonstrate the power of Python libraries for Machine Learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dython.nominal import associations\n",
    "import csv\n",
    "import scipy.stats as stats\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ancestry-train.txt', sep= \" \", header=None)\n",
    "df.columns = ['SNP1', 'SNP2', 'SNP3', 'SNP4','SNP5', 'SNP6', 'SNP7', 'SNP8', 'SNP9', 'SNP10', 'SNP11', 'SNP12', 'SNP13', 'SNP14', 'SNP15', 'SNP16', 'SNP17', 'SNP18', 'SNP19', 'SNP20', 'Ethnicity']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing the colon glitch for SNP20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix(x):\n",
    "    x = x[0]\n",
    "    return int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SNP20'] = df['SNP20'].apply(fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A- \n",
    "Visual data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Ethnicity').mean().plot(kind='bar', yerr = 1.96*df.groupby('Ethnicity').std()/np.sqrt(len(df)))\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = df.groupby('Ethnicity').mean() # This creates two rows, one for each value in the groupby clause. \n",
    "# Calling mean() on this groupby object calculates the mean for each numeric column. \n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.diff() #this line gives the difference between each row of the df and the preceding one.\n",
    "df1.reset_index() #this is needed to create an index based onthe group names (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The point of doing this was to see which of the input features have comparatively more different mean \n",
    "# in two class groups. e.g. SNP20 (-0.345592)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sort_values(by = 1, axis=1, ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.Ethnicity, df.SNP20) # this is a normal crosstab summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.Ethnicity, df.SNP20, normalize='index') # and this is a normalised one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Ethnicity').mean().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding relevance visually (and also by the diff())\n",
    "We can notice that SNP18,SNP12,SNP20,SN16 are all more different in two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(Prediction = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will get a SettingWithCopyWarning warning. You can ignore it. It's a warning, not an error.\n",
    "for (row_index, row_data) in df.iterrows():\n",
    "    if(df['SNP18'][row_index] == 1):\n",
    "        df['Prediction'][row_index] = 0\n",
    "    elif(df['SNP12'][row_index] == 0):\n",
    "        df['Prediction'][row_index] = 1\n",
    "    elif((df['SNP12'][row_index] == 1) & (df['SNP20'][row_index] == 0) & (df['SNP16'][row_index] == 0)):\n",
    "        df['Prediction'][row_index] = 1\n",
    "    elif((df['SNP12'][row_index] == 1) & (df['SNP20'][row_index] == 1)):\n",
    "        df['Prediction'][row_index] = 0\n",
    "    elif(np.random.rand() > 0.5):\n",
    "        df['Prediction'][row_index] = 0\n",
    "    else:\n",
    "        df['Prediction'][row_index] = 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['Ethnicity'], df['Prediction']) #When you run this, you basically get the Confusion Matrix.\n",
    "# We get 40 FPs, 17 FNs as errors\n",
    "# P = 160/200 = 0.8, R = 160/177 = 0.90, F1 =  0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B- \n",
    "Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Os and 1s are not really numbers. They are binary codes given to classes. Let's get rid of them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Ethnicity\"]=df.apply(lambda row: \"East Asian\" if row[\"Ethnicity\"] == 0 else \"Mixed American\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(0,\"N\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(1,\"Y\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heatmap: I make use of an excellent library called dython. associations is a function in it\n",
    "heat_map = associations(df, theil_u=True, figsize=(14, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can see that the relevance is high for SNP18, SNP20, SNP12, SNP11,SNP7,SNP1,SNP6,SNP8 (in descending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One surprising result is that SNP16, SNP15 are strongly correlated mutually (but not to ethnicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we write a programme to take all these features into consideration. \n",
    "# I chose the coefficients arbitrarily (the sqare root of correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"p_r_f1_threshold.csv\" \n",
    "# I am creating a csv file in which I will temporarily store the P,R,F1 values for my 3 iterations of threshold.\n",
    "# (I did a lot of manual up and down with thresholds to arrive at these values which illustrate cut-off points.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, 'w+', newline='') as csvfile:\n",
    "    csvrow = csv.writer(csvfile, delimiter=',')\n",
    "    csvrow.writerow([\"Threshold\",\"TP (testing for '0')\",\"FN\",\"FP\",\"TN\",\"Precision\",\"Recall\",\"F1\"])\n",
    "    for threshold in [.12,.25,.42]:\n",
    "        for (row_index, row_data) in df.iterrows():\n",
    "            predicted = 0\n",
    "            if(df['SNP18'][row_index] == \"N\"):\n",
    "                predicted = predicted + 0.128761783227184\n",
    "            if(df['SNP20'][row_index] == \"N\"):\n",
    "                predicted = predicted + 0.118075965756857\n",
    "            if(df['SNP12'][row_index] == \"N\"):\n",
    "                predicted = predicted + 0.113187926182593\n",
    "            if(df['SNP11'][row_index] == \"Y\"):\n",
    "                predicted = predicted + 0.113187926182593\n",
    "            if(df['SNP7'][row_index] == \"Y\"):\n",
    "                predicted = predicted + 0.0970578459776468\n",
    "            if(df['SNP1'][row_index] == \"N\"):\n",
    "                predicted = predicted + 0.0970578459776468\n",
    "            if(df['SNP6'][row_index] == \"Y\"):\n",
    "                predicted = predicted + 0.091048330077614\n",
    "            if(df['SNP8'][row_index] == \"N\"):\n",
    "                predicted = predicted + 0.0889549851963864\n",
    "            if(df['SNP4'][row_index] == \"Y\"):\n",
    "                predicted = predicted + 0.0800359501521536\n",
    "            if(df['SNP5'][row_index] == \"Y\"):\n",
    "                predicted = predicted + 0.0726314412693259\n",
    "            if predicted > threshold:\n",
    "                df['Prediction'][row_index] = \"Mixed American\"\n",
    "            else:\n",
    "                df['Prediction'][row_index] = \"East Asian\"\n",
    "        predictions = pd.crosstab(df['Ethnicity'], df['Prediction'])\n",
    "        tp = predictions.iloc[0,0]\n",
    "        tn = predictions.iloc[1,1]\n",
    "        fn = predictions.iloc[0,1]\n",
    "        fp = predictions.iloc[1,0]\n",
    "        p = round(tp/(tp+fp),2)\n",
    "        r = round(tp/(tp+fn),2)\n",
    "        f1 = round(2*p*r/(p+r),2)\n",
    "        csvrow.writerow([threshold,tp,fn,fp,tn,p,r,f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_as_p_and_r_tradeoff = pd.read_csv(\"p_r_f1_threshold.csv\")\n",
    "f1_as_p_and_r_tradeoff "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notice how we can have trade-off between FPs and FNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_as_p_and_r_tradeoff[[\"Precision\",\"Recall\",\"F1\"]].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C-\n",
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will again need binary number coding\n",
    "df = pd.read_csv('ancestry-train.txt', sep= \" \", header=None)\n",
    "df.columns = ['SNP1', 'SNP2', 'SNP3', 'SNP4','SNP5', 'SNP6', 'SNP7', 'SNP8', 'SNP9', 'SNP10', 'SNP11', 'SNP12', 'SNP13', 'SNP14', 'SNP15', 'SNP16', 'SNP17', 'SNP18', 'SNP19', 'SNP20', 'Ethnicity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SNP20'] = df['SNP20'].apply(fix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual train-test split using numpy's random.rand\n",
    "mask = np.random.rand(len(df)) < 0.9\n",
    "train = df[mask]\n",
    "test = df[~mask]\n",
    "print('Train Shape: {}\\nTest Shape: {}'.format(train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_Mixed_Americans = train.Ethnicity.value_counts()[1]\n",
    "print('There are {} Mixed American in the train data.'.format(no_of_Mixed_Americans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly selecting 93 random East Asians \n",
    "# (The number 93 will change everytime you run the code in Line 45 due to the rand )\n",
    "east_Asians = train[train['Ethnicity'] == 0]\n",
    "mixed_Americans = train[train['Ethnicity'] == 1]\n",
    "selected = east_Asians.sample(no_of_Mixed_Americans)\n",
    "selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenating both into a subsample data set with equal class distribution\n",
    "selected.reset_index(drop=True, inplace=True)\n",
    "mixed_Americans.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = pd.concat([selected, mixed_Americans])\n",
    "len(subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = subsample.sample(frac=1).reset_index(drop=True)\n",
    "subsample.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = subsample.drop('Ethnicity', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = subsample['Ethnicity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-SNE\n",
    "X_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE scatter plot\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "f, ax = plt.subplots(figsize=(24,16))\n",
    "\n",
    "\n",
    "blue_patch = mpatches.Patch(color='#0A0AFF', label='East Asian')\n",
    "red_patch = mpatches.Patch(color='#AF0000', label='Mixed American')\n",
    "\n",
    "ax.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='East Asian', linewidths=2)\n",
    "ax.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Mixed American', linewidths=2)\n",
    "ax.set_title('t-SNE', fontsize=14)\n",
    "\n",
    "ax.grid(True)\n",
    "\n",
    "ax.legend(handles=[blue_patch, red_patch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_validation = X_test.values\n",
    "y_train = y_train.values\n",
    "y_validation = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_shapes:\\n', 'X_train:', 'X_validation:\\n', X_train.shape, X_validation.shape, '\\n')\n",
    "print('Y_shapes:\\n', 'Y_train:', 'Y_validation:\\n', y_train.shape, y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=42)\n",
    "cv_results = cross_val_score(LogisticRegression(), X_train, y_train, cv=kfold, scoring='roc_auc')\n",
    "name = \"LR\"\n",
    "msg = '%s: %f (%f)' % (name, cv_results.mean(), cv_results.std())\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr = LogisticRegression()\n",
    "logreg = logisticRegr.fit(X_train, y_train) #fitting the model with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(logreg.coef_) #These are the coefficients for the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating\n",
    "cnf_matrix = confusion_matrix(y_test,y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = cnf_matrix[0][0]/(cnf_matrix[0][0]+cnf_matrix[1][0])\n",
    "recall = cnf_matrix[0][0]/(cnf_matrix[0][0]+cnf_matrix[0][1])\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print(f\"Precision = {round(precision,2)}, Recall = {round(recall,2)}, F1 = {round(f1,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "Though it may depend on the random samples that you got, I found the accuracy of the LogisticRegression model better everytime I ran the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
